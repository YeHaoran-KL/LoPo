{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [*] Loading model from pretrained_LCP/Reviser-6-FI/reviser_20/epoch-199.pt\n"
     ]
    }
   ],
   "source": [
    "reviser_path = f'pretrained_LCP/Reviser-6-FI/reviser_20/epoch-199.pt'\n",
    "reviser, _ = load_model(reviser_path, is_local=True, mtl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f'data/FI_train_tsp/200_FI20_seed1235.pkl'\n",
    "dataset = reviser.problem.make_dataset(filename=dataset_path, num_samples=200000, offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import reconnect\n",
    "def load_problem(name):\n",
    "    from problems import TSP, CVRP, SDVRP, OP, PCTSPDet, PCTSPStoch,LOCAL\n",
    "    problem = {\n",
    "        'local': LOCAL,\n",
    "        'tsp': TSP,\n",
    "        'cvrp': CVRP,\n",
    "        'sdvrp': SDVRP,\n",
    "        'op': OP,\n",
    "        'pctsp_det': PCTSPDet,\n",
    "        'pctsp_stoch': PCTSPStoch,\n",
    "    }.get(name, None)\n",
    "    assert problem is not None, \"Currently unsupported problem: {}!\".format(name)\n",
    "    return problem\n",
    "problem = load_problem('local')\n",
    "get_cost_func = lambda input, pi: problem.get_costs(input, pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.2946]), None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = dataset[5].unsqueeze(0)\n",
    "ini_pi = torch.arange(20)\n",
    "get_cost_func(test_sample, ini_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.2336]),\n",
       " tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 15, 16, 17, 14, 13, 12, 11,\n",
       "          18, 19]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviser.eval()\n",
    "reviser.set_decode_type(\"greedy\")\n",
    "_, pi = reviser(test_sample, return_pi=True)\n",
    "_, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_sample.gather(1, pi.unsqueeze(-1).expand_as(test_sample)).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1858/200000 [00:27<49:11, 67.12it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yhr/yhr/LCP-main/generate_data_R.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B116.63.221.103/home/yhr/yhr/LCP-main/generate_data_R.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m ini_pi \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m20\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B116.63.221.103/home/yhr/yhr/LCP-main/generate_data_R.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ini_cost, _ \u001b[39m=\u001b[39m get_cost_func(sample, ini_pi)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B116.63.221.103/home/yhr/yhr/LCP-main/generate_data_R.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m re_cost, re_pi \u001b[39m=\u001b[39m reviser(sample, return_pi\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B116.63.221.103/home/yhr/yhr/LCP-main/generate_data_R.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m re_cost \u001b[39m<\u001b[39m ini_cost:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B116.63.221.103/home/yhr/yhr/LCP-main/generate_data_R.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     re_sample \u001b[39m=\u001b[39m sample\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m, re_pi\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand_as(sample))\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/envs/Attention/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/yhr/LCP-main/nets/attention_local.py:178\u001b[0m, in \u001b[0;36mAttentionModel.forward\u001b[0;34m(self, input, return_pi)\u001b[0m\n\u001b[1;32m    174\u001b[0m     embeddings, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_embed(\u001b[39minput\u001b[39m))\n\u001b[1;32m    175\u001b[0m \u001b[39m# embeddings shape: (batch size (i.e. width x decomposed pieces), problem size, embedding size)\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[39m# _log_p, pi, entropies = self._inner(input, embeddings)\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m _log_p, pi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inner(\u001b[39minput\u001b[39;49m, embeddings)\n\u001b[1;32m    179\u001b[0m \u001b[39m# entropies = entropies.mean(1)\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[39m# for inference, inverse coordinate transformation\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m return_pi:\n",
      "File \u001b[0;32m~/yhr/LCP-main/nets/attention_local.py:308\u001b[0m, in \u001b[0;36mAttentionModel._inner\u001b[0;34m(self, input, embeddings)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39m# Select the indices of the next nodes in the sequences, result (batch_size) long\u001b[39;00m\n\u001b[1;32m    306\u001b[0m selected \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_node(log_p\u001b[39m.\u001b[39mexp()[:, \u001b[39m0\u001b[39m, :], mask[:, \u001b[39m0\u001b[39m, :])  \u001b[39m# Squeeze out steps dimension\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m state \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39;49mupdate(selected)\n\u001b[1;32m    310\u001b[0m \u001b[39m# Now make log_p, selected desired output size by 'unshrinking'\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshrink_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m state\u001b[39m.\u001b[39mids\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m<\u001b[39m batch_size:\n",
      "File \u001b[0;32m~/yhr/LCP-main/problems/local/state_local.py:90\u001b[0m, in \u001b[0;36mStateLOCAL.update\u001b[0;34m(self, selected)\u001b[0m\n\u001b[1;32m     88\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengths\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_coord \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# Don't add length for first action (selection of start node)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     lengths \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlengths \u001b[39m+\u001b[39;49m (cur_coord \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcur_coord)\u001b[39m.\u001b[39;49mnorm(p\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)  \u001b[39m# (batch_dim, 1)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m# Update should only be called with just 1 parallel step, in which case we can check this way if we should update\u001b[39;00m\n\u001b[1;32m     93\u001b[0m first_a \u001b[39m=\u001b[39m prev_a \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mi\u001b[39m.\u001b[39mitem() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_a\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "dataset_RG = []\n",
    "bs = 5\n",
    "for pid in tqdm(range(10)):\n",
    "    sample = dataset[pid].unsqueeze(0)\n",
    "    ini_pi = torch.arange(20)\n",
    "    ini_cost, _ = get_cost_func(sample, ini_pi)\n",
    "\n",
    "    re_cost, re_pi = reviser(sample, return_pi=True)\n",
    "    if re_cost < ini_cost:\n",
    "        re_sample = sample.gather(1, re_pi.unsqueeze(-1).expand_as(sample)).squeeze()\n",
    "        dataset_RG.append(re_sample)\n",
    "    else:\n",
    "        dataset_RG.append(sample.squeeze())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('Attention')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24ad11462de7308ca01150a2416426711c68202aa5a00e49b6cc6c1e480f6355"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
